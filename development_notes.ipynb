{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "I'm adding this notebook to my PRPT repo as of mid-2022 (20220612, to be exact) in order to see how lessons learned from developing this pipeline in mid-2020 can avail me as I try to stand up a similar pipeline in mid-2022.  We have a mandate at the WA PHL to get *something* in place to analyze HAI/AR samples in-house while we await national guidance on a more streamlined/harmonized pipeline for that purpose that could be employed across the states.\n",
    "\n",
    "I've been developing such in another repo (for now private, since it's based off another dev's work, and I haven't obtained her permission to publish it), but in pursuing that I came to a similar architecture as employed by this project.  Namely, in the PRPT exercise, I created one long script that pulled in various tools for the pipeline from Docker images maintained elsewhere (mostly, the [StaPH-B Docker builds repo](https://github.com/StaPH-B/docker-builds)) and executes them in order.  The aim for this new-and-improved HAI pipeline is to eventually run it on the Terra platform, and so we *will* need it containerized, and I had originally thought that would require packaging everything into one common Docker image.  But some of the dependencies in turn have enormous databases on which they draw, and sought guidance on whether there are upper limits on what constitutes a decent container.  I found [this page](https://docs.dockstore.org/en/stable/advanced-topics/best-practices/best-practices-secure-fair-workflows.html), and it says:\n",
    "\n",
    "<blockquote>A good rule of thumb is that each image should have a specific purpose. Avoid installing all of the software you need for an entire analysis in one container, instead use multiple containers.</blockquote>\n",
    "\n",
    "So the approach ends up being pretty much what I employed for the PRPT exercise two years ago, anyways.  To that end, I thought that it would be useful to take another look at this repo, modernize it a bit, and see how it performs as-is versus the pipeline we're developing.  Several of the tools are similar, but the new pipeline that was developed by Hannah Gray differs in that it has several more tools included, and some custom DBs, and finally that it was all orchestrated in a Bash script, a script was only ever written to work on Hannah's EC2 instance.  Therefore it's *incredibly* particular and not generalizeable, with many paths to tools and resources hard-coded into the script.  As I'm modifying hers, my instinct is to transfer all the critical steps over to a Python script or perhaps eventually a true package.  But given how similar the appropriate strategy appears to be, I thought it would be instructive to take another look at this older repo, and what I learned in designing it to date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modernizing the PRPT Script\n",
    "\n",
    "Ok, first off, I cloned the old repo to my main EC2 instance, and chosen to open it in VS Code (my original development was done in classic browser-based Jupyter notebook servers).  I've added a VS Code workspace file for configuration, and will want to include `launch.json` files to partially automate testing.\n",
    "\n",
    "One of the first things that I notice is that the expectation for read inputs is that they will have the \"`.zip`\" extension and need to be extracted, but the example inputs that I have for adapting Hannah's HAI script are all \"`.fastq.gz`\", which I believe is the more common format, anyways.  I since discovered that Python's standard library [appears to contain a `gzip` package](https://docs.python.org/3/library/gzip.html) that would be the better way to handle it.  The docs contain an example of how to use it to decompress files, which looks like it should retain the original, compressed file in place, too.\n",
    "\n",
    "...Never mind, I find that, in truth, I already knew about that package, and invoked it in the script.  It's just that it wasn't used to extract reads, but rather a file downloaded from the NCBI FTP site.  So there's nothing new under the sun, just a package that I had forgotten about in the intervening two years.  I just need to implement the suggested code in the... never mind again?!?  I guess I was looking at the \"`unzip_fastqc`\" function, which deals not with the raw reads but with the output of FastQC summary of quality...  I guess that the major tools used in this pipeline were always compatible with gzipped read files?  Or else I had extracted them by some step executed in the terminal, and outside the context of my old dev notes.  Ignore this change for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up an Environment\n",
    "\n",
    "But maybe I'm getting ahead of myself.  I suppose first I need to recognize the dependencies of the original PRPT script, and build a new conda env containing them all.  The beauty of this approach is that there shouldn't be many, as the programs actually used for bioinformatic analysis weren't installed locally, but used via containers, as mentioned, so it should just be a few Python packages not included in the standard library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import argparse\n",
      "import datetime\n",
      "import docker\n",
      "import gzip\n",
      "import json\n",
      "import os\n",
      "import pandas as pd\n",
      "import re\n",
      "import shlex\n",
      "import shutil\n",
      "import subprocess\n",
      "import sys\n"
     ]
    }
   ],
   "source": [
    "with open('prpt_script_1.py', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if line[:7] == \"import \":\n",
    "            print(line, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like just \"`docker`\" and \"`pandas`\".  Nice and neat.  I just added \"`docker-py`\" to my \"`jupyter`\" conda env, rather than creating a brand new one for this.\n",
    "\n",
    "My next step was going to be to set up the \"`launch.json`\" file to try the script as currently configured, but I see instead that one of the first args in the script is a path to a local installation of SNVPhyl.  I guess that's one dependency that didn't exist in containerized form when I was last looking.  There's currently a [containerized version of at least some of the tools](https://github.com/StaPH-B/docker-builds/tree/master/snvphyl-tools/1.8.2) that SNVPhyl relies upon, maintained by StaPH-B.  It looks like they unpacked it from the original script, and say they mean for it to be invoked via Nextflow; I'll have to see if a local clone of the SNVPhyl repo or some part of it will be required to finish the rest...\n",
    "\n",
    "Well, it's too early to get too caught up in that.  Hannah's script doesn't use SNVPhyl; we can choose later whether to handle it somehow.  For now, stick with a local installation.  I see from my \"`PRPT_follow-up_20210416_freeze.html`\" version of an old notebook (the original of which is now lost), that I had used the version from [the \"`cli`\" version of the main SNVPhyl repo](https://github.com/phac-nml/snvphyl-galaxy-cli#snvphyl-cli), and that I had trouble getting it to run, until I found out that the program apparently needs input read files renamed to the form \"`{sample}_1.fastq`\"...  I'll have to see if that's still the case.\n",
    "\n",
    "...Hmm, I see that the installation instructions for that repo involve doing a `pip install -r requirements.txt` step, but the only package within is \"`bioblend`\", so it's also probably compatible with an existing conda env.  Ok, I added bioblend and docker-py to my \"`gray`\" conda env; run this one from there.  ...I also subsequently found that `boto3` is an optional dependency, if you want to ask the script to upload some results to an S3 bucket.\n",
    "\n",
    "I'll run `git clone https://github.com/phac-nml/snvphyl-galaxy-cli` from `/data/hai`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Ok, my first test of the script was going well until it hit the `get_ftp_file` func, where it stalled on line 553 with the error:\n",
    "\n",
    "`Exception has occurred: error_perm\n",
    "550 /genomes/refseq/bacteria/Citrobacter_sp/: No such file or directory`\n",
    "\n",
    "I tried accessing the NCBI FTP site directly, and was having issues with Firefox not opening URLs that begin with \"`ftp://`\", and being redirected to the WinSCP program, which wants login credentials.  Instead, I finally found a page that redirected me to the \"NCBI's anonymous FTP site\", and the trick is that you use the FTP URL, but preface it with the \"`https://`\" to indicate that you just want to open it in a browser with the hypertext protocol.  Now I'm trying to open the \"`/genomes/refseq/bacteria`\" page, but it's hanging up indefinitely on me.  Unsure whether it's just trying to load a **REALLY** huge page, or if something else is going wrong.\n",
    "\n",
    "Ok, anyways, it finally loaded, and it looks like somethings just going wrong with the logic to parse the species names; there are multiple \"Citrobacter_sp\" entries, including, well, looks like scores of them.  Probably choosing poor logic for a string split to get the URL, because my original test samples didn't include any \"Citrobacter_sp\" to work with.  When I relaunched to troubleshoot, I decided to pare down the input sample candidates to just the first sample, but that one wasn't the Citrobacter sample.  So I may have to iteratively troubleshoot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But one major issue with troubleshooting is that apparently I wrote this script before I got used to using the `logging` module.  So it's really limited in the info on what is happening that gets written to the terminal, and apparently nothing at all gets stored in terms of metadata about how the script ran as a permanent record via a file among the outputs.\n",
    "\n",
    "And I'll definitely want to add logic to capture which versions of these Docker images are being used, and the versions of the tools within.\n",
    "\n",
    "...Oops; even the first sample failed later on, when trying to save out the combined Abricate dataframe as an Excel file, because I had missed that `openpyxl` is a dependency for that step.  Man, I've really got to get into the practice of putting all import statements up front at the top of the file, instead of buried inside the functions that specifically need them... oh, I see, there is no call to `import openpyxl` in the script, it's just implicit when saving pandas DataFrames out to `.xlsx`.  I don't really know why I had chosen an Excel output, when the [native outputs from abricate are TSVs](https://github.com/tseemann/abricate#output).  I don't know whether I was just trying to make it easier for end users to open with the right program, or if I added any custom formatting (like preserving column widths or something).\n",
    "\n",
    "But since I have to restart, and SPAdes was taking like 10min for one sample, I really need to also add logic that checks for intermediate results, such as the existence of a `scaffolds.fasta` output from SPAdes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipping Assembly\n",
    "\n",
    "Ok, I tried implementing the ability to at least escape the the SPAdes assembly step, but even that functionality is not easy to implement given the way I had originally written the script.  I think I'll need to refactor more significantly before I'll be able to accommodate such a functionality.\n",
    "\n",
    "Specifically, I tried adding a \"`assemblies_dir`\" arg to the `arg_parse` options, then just having the `main` func at the bottom only add the \"`spades_cmds`\" dict to the list of steps to run in \"Part 4\" of that func if no assemblies dir has been passed.  But downstream steps expect the presence of the `scaffolds.fasta` files in the output dir.  So at the very least I need to add logic to look for the scaffolds files for each of the chromosome and plasmid SPAdes output dirs for each of the samples.  But honestly, I think at this point that the exact way I wrote the whole script a couple of years ago constitutes a case of spaghetti code, and a more radical refactor is called for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/data/hai/prpt')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path('').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile('^(.*)[rR][12].*$')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'^(.*)[rR][12].*$'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# if not thing:\n",
    "#     print(\"There's no such a thing, you fool!\")\n",
    "\n",
    "simple_pattern = re.compile(r\"^(.*)[rR][12].*$\")\n",
    "print(simple_pattern)\n",
    "dir(simple_pattern)\n",
    "simple_pattern.pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "import docker\n",
    "\n",
    "CLIENT = docker.from_env()\n",
    "images = CLIENT.images.list()\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.2'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(images[0])\n",
    "images[0].tags\n",
    "images[0].attrs\n",
    "images[0].labels['software.version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base.image': 'ubuntu:focal',\n",
       " 'description': 'The SNVPhyl-tools are for a pipeline for identifying Single Nucleotide Variants (SNV) within a collection of microbial genomes and constructing a phylogenetic tree.',\n",
       " 'dockerfile.version': '1',\n",
       " 'license': 'https://github.com/phac-nml/snvphyl-tools/blob/master/LICENSE',\n",
       " 'maintainer': 'Jill Hagey',\n",
       " 'maintainer.email': 'jvhagey@gmail.com',\n",
       " 'software': 'SNVPhyl-tools',\n",
       " 'software.version': '1.8.2',\n",
       " 'website': 'https://github.com/phac-nml/snvphyl-tools'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].attrs.get('RepoTags')\n",
    "images[0].labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SNVPhyl-tools:1.8.2', 'SPAdes:3.15.4', 'None:None', 'None:None', 'None:None', 'None:None', 'None:None', 'Kraken2:2.1.2', 'None:None', 'None:None', 'None:None', 'None:None', 'iVar:1.3.1', 'None:None', 'Mash:2.3', 'FASTQC:0.11.9', 'Abricate:1.0.0', 'QUAST:5.0.2', 'iVar:1.2.2', 'MultiQC:1.8', 'Trimmomatic:0.39', 'Trimmomatic:0.39']\n"
     ]
    }
   ],
   "source": [
    "local_images = [\n",
    "    f\"{i.labels.get('software')}:{i.labels.get('software.version')}\"\n",
    "    for i in images\n",
    "]\n",
    "print(local_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SNVPhyl-tools:1.8.2',\n",
       " 'SPAdes:3.15.4',\n",
       " 'None:None',\n",
       " 'None:None',\n",
       " 'None:None',\n",
       " 'None:None',\n",
       " 'None:None',\n",
       " 'Kraken2:2.1.2',\n",
       " 'None:None',\n",
       " 'None:None',\n",
       " 'None:None',\n",
       " 'None:None',\n",
       " 'iVar:1.3.1',\n",
       " 'None:None',\n",
       " 'Mash:2.3',\n",
       " 'FASTQC:0.11.9',\n",
       " 'Abricate:1.0.0',\n",
       " 'QUAST:5.0.2',\n",
       " 'iVar:1.2.2',\n",
       " 'MultiQC:1.8',\n",
       " 'Trimmomatic:0.39',\n",
       " 'Trimmomatic:0.39']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Image: 'python:3.8.12-slim'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(repr(images[3]))\n",
    "# images[3].attrs\n",
    "# CLIENT.images.get('trimmomatic')\n",
    "CLIENT.containers.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docker_create(\n",
    "    image_name: str, volumes: dict, logger: logging.Logger, \n",
    "    name=None, tty=True, overwrite=True, **kwargs\n",
    "    ):\n",
    "    \"\"\"Launch a new container image with Docker SDK.\n",
    "    If overwrite=True and a container with the given name\n",
    "    already exists; stop and remove it.\"\"\"\n",
    "    import docker\n",
    "    import os\n",
    "    from functools import partial\n",
    "    CLIENT = docker.from_env()\n",
    "    USER = str(os.getuid())\n",
    "    GID = str(os.getgid())\n",
    "    # Check that image is present\n",
    "    images = CLIENT.images.list()\n",
    "    local_img = CLIENT.images.get(image_name)\n",
    "    if not local_img:\n",
    "        repo, version = image_name.split(\":\")\n",
    "        image_missing_msg = (\n",
    "            f\"Could not find local installation of image: '{repo}'; \"\n",
    "            f\"attempting to download version: '{version}' now.\"\n",
    "        )\n",
    "        logger.info(image_missing_msg)\n",
    "        CLIENT.images.pull(repo, tag=version)\n",
    "\n",
    "    create_func = partial(\n",
    "        CLIENT.containers.create,\n",
    "        group_add=GID,\n",
    "        image=image_name,\n",
    "        name=name,\n",
    "        tty=tty,\n",
    "        volumes=volumes,\n",
    "        user=USER,\n",
    "        **kwargs,\n",
    "    )\n",
    "    try:\n",
    "        container = create_func()\n",
    "    except docker.errors.APIError:\n",
    "        old_container = CLIENT.containers.get(name)\n",
    "        if overwrite:\n",
    "            old_container_msg = (\n",
    "                f\"Found running container: {old_container}\"\n",
    "            )\n",
    "            old_container.stop()\n",
    "            old_container.remove(v=True, force=True)\n",
    "            container = create_func()\n",
    "        else:\n",
    "            container = old_container\n",
    "\n",
    "    return container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NullResource",
     "evalue": "Resource ID was not provided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/python/envs/gray/lib/python3.6/site-packages/docker/api/client.py\u001b[0m in \u001b[0;36m_raise_for_status\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/envs/gray/lib/python3.6/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: http+docker://localhost/v1.35/containers/create",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dd3343a4ee2b>\u001b[0m in \u001b[0;36mdocker_create\u001b[0;34m(image_name, volumes, logger, name, tty, overwrite, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mdocker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/envs/gray/lib/python3.6/site-packages/docker/models/containers.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, image, command, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0mcreate_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_container_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_container\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcreate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/envs/gray/lib/python3.6/site-packages/docker/api/container.py\u001b[0m in \u001b[0;36mcreate_container\u001b[0;34m(self, image, command, hostname, user, detach, stdin_open, tty, ports, environment, volumes, network_disabled, name, entrypoint, working_dir, domainname, host_config, mac_address, labels, stop_signal, networking_config, healthcheck, stop_timeout, runtime)\u001b[0m\n\u001b[1;32m    409\u001b[0m         )\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_container_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/envs/gray/lib/python3.6/site-packages/docker/api/container.py\u001b[0m in \u001b[0;36mcreate_container_from_config\u001b[0;34m(self, config, name)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/envs/gray/lib/python3.6/site-packages/docker/api/client.py\u001b[0m in \u001b[0;36m_result\u001b[0;34m(self, response, json, binary)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/envs/gray/lib/python3.6/site-packages/docker/api/client.py\u001b[0m in \u001b[0;36m_raise_for_status\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mcreate_api_error_from_http_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/envs/gray/lib/python3.6/site-packages/docker/errors.py\u001b[0m in \u001b[0;36mcreate_api_error_from_http_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNotFound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplanation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplanation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAPIError\u001b[0m: 400 Client Error: Bad Request (\"create data/hai/prpt_test_1/20220614-0642_PRPT: \"data/hai/prpt_test_1/20220614-0642_PRPT\" includes invalid characters for a local volume name, only \"[a-zA-Z0-9][a-zA-Z0-9_.-]\" are allowed. If you intended to pass a host directory, use absolute path\")",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNullResource\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-44c30cb5e49d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prpt_test_logger'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdocker_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'staphb/trimmomatic:latest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolumes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-dd3343a4ee2b>\u001b[0m in \u001b[0;36mdocker_create\u001b[0;34m(image_name, volumes, logger, name, tty, overwrite, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mdocker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mold_container\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLIENT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             old_container_msg = (\n",
      "\u001b[0;32m~/python/envs/gray/lib/python3.6/site-packages/docker/models/containers.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, container_id)\u001b[0m\n\u001b[1;32m    841\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mserver\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0man\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \"\"\"\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minspect_container\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/envs/gray/lib/python3.6/site-packages/docker/utils/decorators.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, resource_id, *args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 raise errors.NullResource(\n\u001b[0;32m---> 17\u001b[0;31m                     \u001b[0;34m'Resource ID was not provided'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 )\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNullResource\u001b[0m: Resource ID was not provided"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "# from prpt_script_1 import docker_create\n",
    "\n",
    "volumes = {\n",
    "    '/data/hai/prpt_test_1/input_reads_2': \n",
    "    {'bind': '/inputs', 'mode': 'ro'},\n",
    "    'data/hai/prpt_test_1/20220614-0642_PRPT':\n",
    "    {'bind': '/data', 'mode': 'rw'}\n",
    "}\n",
    "logger = logging.Logger('prpt_test_logger')\n",
    "docker_create('staphb/trimmomatic:latest', volumes, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image: 'staphb/trimmomatic:latest'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import docker\n",
    "\n",
    "CLIENT = docker.from_env()\n",
    "CLIENT.images.pull('staphb/trimmomatic:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "with open('/home/joe/file.txt', 'rb') as f_in:\n",
    "    with gzip.open('/home/joe/file.txt.gz', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
